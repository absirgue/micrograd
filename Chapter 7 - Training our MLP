# Chapter 7: Training our MLP

We have now come to the point where we can assemble neurons in multi-layer neural perceptrons.

The behavior of these MLPs, however, is completely heratic for now. This is because we have set the weights to random values and have no way of adjusting those values to enable our overall network to get closer to the expected answer.

To be able to adjust those weights, Deep Learning uses a trick. We are going to introduce the loss function which returns a single number that measures the total performance of our Neural Network.

## Creating the Loss Function

To create the Loss Function, we need some measurment of our network's performance. One way to measure our network's performance is by using its Mean Squared Error.

## Backpropagating from the Loss Function

If we now backpropagate from the result of the loss function, we can calculate the gradient of each of our neuron's weights. This number tells the "direction" and "strength" of this weight's influence on the loss.

With this information, we are able to take all of the network's weights and biases and to nudge them by a small amount depending on the value of their gradient.

Our idea when adjusting each parameter's value is to take a very small step in the direction opposit of its gradient. We can see its gradient as a vector and our goal is to move by a very small amount in the direction opposit of that vector. We move in the opposit direction because our goal is to minimize the loss. Hence, if the gradient is negative, we need a positive change in the parameter's value to make it lower the loss, and if it is positive, we want a negative change to make it lower the loss. This is called **Gradient Descent**.
