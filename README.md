# The Core Concepts of Neural Networks by Andrej Karpathy

The [first episode of Andrej Karpathy's series of videos on Neural Networks](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2) focuses on building out micrograd, a very simple Neural Network library. Building this library is a fantastic opportunity to get down to the mathematical core of neural networks and to manually implement key concepts such as backpropagation and gradient descent.

This repository presents the notes, learning exercises, and final project covered in this video.

## Navigating the repository

- _./micrograd contains_ the code for this video's final project, building micrograd
- _./learning_exercises_ presents all the exercises done along the way, this folder contains a lot of code duplication as its goal is to present the state of classes at the time when a particular exercise was done. Classes are therefore rewritten each time. The exercises are organized in chapters.
- _./notes_ contains my notes from the video. The notes are organized in chapters.
- _./illustrations_ contains images inserted in the notes.

## Source

All of the code of this repository directly comes from or is derived from Andrej Karpathy's work on [micrograd](https://github.com/karpathy/micrograd)

**Andrej Karpathy**
Andrej Karpathy (born 23 October 1986) is a Slovak-Canadian computer scientist who served as the director of artificial intelligence and Autopilot Vision at Tesla. He currently works for OpenAI, where he specializes in deep learning and computer vision. _(Wikipedia)_
